{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab05: Introduction to Partial Observability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore some most basic ideas that are classic in partial observable environments and try to deploy that knowledge onto simple games such as lemonade. The lab is consisted of two parts. In the first one you will learn about POMDP and belief states update, while in the second one you will learn to implement a simple recurrent Q-learning agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Partially Observable Markov Decision Process(POMDP), the agent does not directly observe the underlying true state of the environment but instead makes an observation o at each timestep. The probability of observing o given an action a and transitioned state s' is denoted by O(o|a, s'). And the transition function between the true states is given by T(s'|s, a). In this lab, we will focus on discrete states, and so a suitable inference known as recursive Bayesian estimation can be used to maintain ::our belief:: about the current states we are in. When the state and observation spaces are finite, we can use maintain a categorical distribution of length |S| and we call it a 'belief vector'. Sometimes it is also referred to as a probability simplex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem:\n",
    "The belief updating function, given a fixed observation function O(o|a,s), a transition function T and a previous belief state b, is given by b'(s') = O(o|a,s') $\\sum_s $ T(s'|s,a)b(s). \n",
    "\n",
    "**TO-DO: Try to derive this formula by yourself!**\n",
    "\\begin{align*}\n",
    "b'(s') &= P(s'|b,a,o) \\\\\n",
    "       &= P(o|b,a,s')P(s'|b,a) \\\\\n",
    "       &= O(o|a,s')P(s'|b,a) \\\\\n",
    "       &=  \\\\\n",
    "       &= O(o|a,s') \\sum_s T(s'|s,a)b(s)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Then, the belief vector is dived by a constant N so that it's normalized as a proper simplex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: model for a simple fixed opponent in Lemonade ####\n",
    "\n",
    "In the previous lab we did the lemonade stand game, where you compete against two agents with unknown types. Now we simplify the game to be only you competing against ONE agent with a FIXED strategy. This strategy is:\n",
    "\n",
    "The agent has two potential types: one we call ***type3*** when it will play position 3 with a probability of 70% and position 9 30%, another is ***type9*** when it plays position 9 70% of the time and position 3 30%. At the start of the game, the agent is going to be in either one of the type, and it's never going to change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try just using a random strategy against this agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agt_server.agents.base_agents.lemonade_agent import LemonadeAgent\n",
    "from agt_server.local_games.lemonade_arena import LemonadeArena\n",
    "from agt_server.agents.test_agents.lemonade.stick_agent.my_agent import StickAgent\n",
    "from agt_server.agents.test_agents.lemonade.always_stay.my_agent import ReserveAgent\n",
    "from agt_server.agents.test_agents.lemonade.decrement_agent.my_agent import DecrementAgent\n",
    "from agt_server.agents.test_agents.lemonade.increment_agent.my_agent import IncrementAgent\n",
    "\n",
    "# first let's make a dummy bot that always pick position 0\n",
    "# So we can have an environment of only two people competing\n",
    "class DummyBot(LemonadeAgent):\n",
    "    def setup(self):\n",
    "        self.spot = 0\n",
    "    def get_action(self):\n",
    "        return self.spot\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "# second let's code up our Mysterious agent we just described\n",
    "import random\n",
    "class MysteriousBot(LemonadeAgent):\n",
    "    def setup(self):\n",
    "        self.agent_type = random.choice(['type3', 'type9'])\n",
    "\n",
    "        if self.agent_type == 'type3':\n",
    "            self.probs = {3: 0.7, 9: 0.3}\n",
    "        else:\n",
    "            # type9\n",
    "            self.probs = {3: 0.3, 9: 0.7}\n",
    "\n",
    "    def get_action(self):\n",
    "        # Choose an action (i.e., a position) based on the fixed probability distribution.\n",
    "        positions = list(self.probs.keys())\n",
    "        weights = list(self.probs.values())\n",
    "        chosen_position = random.choices(positions, weights=weights, k=1)[0]\n",
    "        return chosen_position\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "class RandomAgent(LemonadeAgent):\n",
    "    def setup(self):\n",
    "        pass\n",
    "    def get_action(self):\n",
    "        return random.randint(0, 11)\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "randomagent = RandomAgent(\"Random\")\n",
    "mysteriousagent = MysteriousBot(\"Mysterious\")\n",
    "dummyagent = DummyBot(\"PlaceHolder\")\n",
    "\n",
    "arena = LemonadeArena(\n",
    "    num_rounds=500,\n",
    "    timeout=1, \n",
    "    players=[\n",
    "        randomagent,\n",
    "        mysteriousagent,\n",
    "        dummyagent,\n",
    "    ]\n",
    ")\n",
    "arena.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that an intuition of how to do better in this simple game is to decide which type is your mysterious agent, type3 or type9? With the previously introduced tools(Recursive Bayesian Estimation), you can now try to infer about this information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeliefAgent(LemonadeAgent):\n",
    "    def setup(self):\n",
    "        # Initialize belief vector: state 0 -> type3, state 1 -> type9.\n",
    "        self.b = [0.5, 0.5]\n",
    "        # Transition matrix T: since the type never changes, it is the identity.\n",
    "        self.T = [[1, 0],\n",
    "                  [0, 1]]\n",
    "        # Observation function O: maps state index to a dict of observation probabilities.\n",
    "        # For type3 (state 0): P(3)=0.7, P(9)=0.3.\n",
    "        # For type9 (state 1): P(3)=0.3, P(9)=0.7.\n",
    "        self.O = {\n",
    "            0: {3: 0.7, 9: 0.3},\n",
    "            1: {3: 0.3, 9: 0.7}\n",
    "        }\n",
    "\n",
    "    def get_action(self):\n",
    "        ######### TO-DO: Find the optimial policy in expectation given the current belief state #########\n",
    "        p3 = self.b[0] * self.O[0][3] + self.b[1] * self.O[1][3]\n",
    "        p9 = self.b[0] * self.O[0][9] + self.b[1] * self.O[1][9]\n",
    "        \n",
    "        best_utility = float('-inf')\n",
    "        best_action = None\n",
    "        # Loop over all potential actions (0 to 11)\n",
    "        for a in range(12):\n",
    "            # calculate_utility should compute the expected utility for taking action `a`\n",
    "            # given the predicted probabilities for agent2's behavior (p3 and p9).\n",
    "            # Here we assume it returns a numeric utility value.\n",
    "            utility = p3 * self.calculate_utility(a, 0, 3)[0] + p9 * self.calculate_utility(a, 0, 9)[0]\n",
    "            if utility > best_utility:\n",
    "                best_utility = utility\n",
    "                best_action = a\n",
    "        #################################################################################################\n",
    "        return best_action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update the belief vector given an observation from agent1.\n",
    "        `observation` should be either 3 or 9.\n",
    "        \n",
    "        The belief update is:\n",
    "            b'(s') = O(observation | s') * b(s') / N\n",
    "        where N = sum_{s''} O(observation | s'') * b(s'').\n",
    "        \"\"\"\n",
    "        if not(self.get_opp1_last_action()):\n",
    "            pass\n",
    "        observation = self.get_opp1_last_action()\n",
    "        new_b = [0.0, 0.0]\n",
    "\n",
    "        ############ TO_DO: Update new_b to be your new belief state based on the observation! #########\n",
    "        for s in range(2):\n",
    "            # Because the transition is identity, we simply multiply the prior by the observation likelihood.\n",
    "            new_b[s] = self.O[s][observation] * self.b[s]\n",
    "        # Normalize the new belief so that it sums to 1.\n",
    "        total = sum(new_b)\n",
    "        if total > 0:\n",
    "            self.b = [x / total for x in new_b]\n",
    "        else:\n",
    "            # Fallback: if something went wrong (total==0), reset to uniform belief.\n",
    "            self.b = [0.5, 0.5]\n",
    "        #################################################################################################\n",
    "\n",
    "beliefagent = BeliefAgent(\"belief\")\n",
    "arena = LemonadeArena(\n",
    "    num_rounds=500,\n",
    "    timeout=1, \n",
    "    players=[\n",
    "        beliefagent,\n",
    "        mysteriousagent,\n",
    "        dummyagent,\n",
    "    ]\n",
    ")\n",
    "arena.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TO-DO: ***\n",
    "\n",
    "Suppose your start with [0.5, 0.5] as your belief vector. You observed that your opponent played 3, and 9, and 3 again. What will be your belief about the type of your opponent? Work by hands. \n",
    "\n",
    "\n",
    "[ANSWER]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Handle Partial Observability with Memory-based RL\n",
    "\n",
    "In this part of the lab, you'll be implementing a recurrent Q-learning agent, with a gated recurrent unit (GRU) as the recurrent network. Let's first implement the RNN Q-function. In previous assignments, you implemented Q-functions as a linear function between state features and weights, $Q(\\phi(s_t), a_t) = \\mathbf{w}_a^{\\intercal}\\phi(s_t)$. Now we'll be using a *recurrent neural network* as our Q-function. If you need a good reference to how recurrent neural networks work, we refer you to page 370 of the [deep learning book](https://www.deeplearningbook.org/contents/rnn.html). Let $Q_{\\theta} : \\mathcal{O} \\times \\mathcal{A} \\times \\mathcal{H} \\rightarrow \\mathbb{R}^{|A|} \\times \\mathcal{H}$ represent our recurrent Q-function neural network, parameterized by $\\theta$. Our Q-function takes as input the previous action taken, the current observation seen, and the previous hidden state as input. Here, $\\mathcal{H}$ represents the space of our hidden states (normally $\\mathbb{R}^d$, where $d$ is the dimension of your hidden state), which is meant to be a condensed version of our history of observations and actions. Our Q-function takes as input the observation $\\mathbf{o}_t$, the previous action $a_{t - 1}$ and previous hidden state $\\mathbf{h}_{t - 1}$ and outputs the Q-values of _all_ actions at time step $t$, as well as the next hidden state $\\mathbf{h}_t$.\n",
    "\n",
    "We'll be using the PyTorch library to implement our neural network. If you need a reference to implementing RNNs, check out this tutorial on [RNN classification with PyTorch](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). If you need a reference to how GRUs work, check out the [PyTorch documentation for GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html).\n",
    "\n",
    "For a Q-Learning agent on Lemonade, refer to the my_rl_lemonade_agent.py and q_learning.py files. \n",
    "\n",
    "\n",
    "**(This Introduction is adapted from CS2951F)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# from my_rl_lemonade_agent import MyRLAgent\n",
    "# from my_supervised_agent import MyNRLAgent\n",
    "\n",
    "from agt_server.agents.base_agents.lemonade_agent import LemonadeAgent\n",
    "from agt_server.local_games.lemonade_arena import LemonadeArena\n",
    "from agt_server.agents.test_agents.lemonade.stick_agent.my_agent import StickAgent\n",
    "from agt_server.agents.test_agents.lemonade.always_stay.my_agent import ReserveAgent\n",
    "from agt_server.agents.test_agents.lemonade.decrement_agent.my_agent import DecrementAgent\n",
    "from agt_server.agents.test_agents.lemonade.increment_agent.my_agent import IncrementAgent\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DRQN (Deep Recurrent Q-Network) Model\n",
    "# -------------------------\n",
    "class DRQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DRQN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # The LSTM processes sequences (here we use sequence length = 1 each step).\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        # Use the last time-step’s output\n",
    "        out = out[:, -1, :]  # shape: (batch, hidden_dim)\n",
    "        q_values = self.fc(out)  # shape: (batch, output_dim)\n",
    "        return q_values, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        # Initialize (h, c) to zeros\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        return (h, c)\n",
    "\n",
    "# -------------------------\n",
    "# Simple Replay Buffer\n",
    "# -------------------------\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('obs', 'action', 'reward', 'next_obs', 'done', 'hidden'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# -------------------------\n",
    "# DRQN Agent Implementation\n",
    "# -------------------------\n",
    "class DRQNAgent(LemonadeAgent):\n",
    "    def setup(self):\n",
    "        # ----- Parameters -----\n",
    "        # Number of possible spots (actions)\n",
    "        self.num_positions = 12  \n",
    "        # Our observation will be the concatenation of three one-hot encodings\n",
    "        # (one per agent). Thus, input_dim = 36.\n",
    "        self.input_dim = 36  \n",
    "        self.hidden_dim = 128\n",
    "        self.output_dim = self.num_positions\n",
    "        \n",
    "        self.gamma = 0.99           # discount factor\n",
    "        self.epsilon = 1.0          # initial epsilon for epsilon-greedy\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995  # decay factor per update\n",
    "        self.batch_size = 32\n",
    "        self.replay_capacity = 10000\n",
    "\n",
    "        # ----- DRQN and Optimizer -----\n",
    "        self.policy_net = DRQN(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "        # In a full implementation you might maintain a target network updated less frequently.\n",
    "        self.target_net = DRQN(self.input_dim, self.hidden_dim, self.output_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "\n",
    "        # ----- Replay Buffer -----\n",
    "        self.replay_buffer = ReplayBuffer(self.replay_capacity)\n",
    "\n",
    "        # ----- RNN Hidden State -----\n",
    "        self.hidden = self.policy_net.init_hidden(batch_size=1)\n",
    "        # We will store the last observation and action to form a transition.\n",
    "        self.last_obs = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def encode_observation(self):\n",
    "        \"\"\"\n",
    "        TO-DO::: \n",
    "        Encode the most recent opponent actions as a 36-dimensional vector.\n",
    "        For each opponent we use a 12-dimensional one-hot encoding.\n",
    "        If no action is available, a zero vector is used.\n",
    "        \"\"\"\n",
    "        obs = []\n",
    "        ############# TO-DO: make one-hot encoding of the LAST action of all three players, put into obs use obs.extend() #########\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Create a tensor of shape (1, 1, input_dim)\n",
    "        obs_tensor = torch.tensor([obs], dtype=torch.float32)  # shape: (1, input_dim)\n",
    "        obs_tensor = obs_tensor.unsqueeze(1)  # shape: (1, 1, input_dim), seq_len=1\n",
    "        \n",
    "        return obs_tensor\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Select an action using an epsilon-greedy policy based on the current observation.\n",
    "        \"\"\"\n",
    "        obs = self.encode_observation()  # shape: (1, 1, input_dim)\n",
    "        # Forward pass through the DRQN to get Q-values\n",
    "        with torch.no_grad():\n",
    "            q_values, self.hidden = self.policy_net(obs, self.hidden)\n",
    "            \n",
    "        ######################### TO-DO: Implement the Epsilon-greedy action selection ###############\n",
    "        action = None\n",
    "        ##############################################################################################\n",
    "\n",
    "        # Store the observation and action for the transition.\n",
    "        self.last_obs = obs  \n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Called at every time step. This method:\n",
    "          1. Obtains the reward and next observation.\n",
    "          2. Stores the transition in the replay buffer.\n",
    "          3. Trains the network using a batch sampled from the replay buffer.\n",
    "          4. Updates the target network and decays epsilon.\n",
    "        \"\"\"\n",
    "        # ---- 1. Get reward and next observation ----\n",
    "        reward = self.get_last_util()\n",
    "        done = self.is_done() if hasattr(self, \"is_done\") else False\n",
    "        next_obs = self.encode_observation()\n",
    "\n",
    "        # Detach hidden state to avoid backpropagating through the entire history.\n",
    "        hidden_detached = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        # ---- 2. Save the transition ----\n",
    "        self.replay_buffer.push(self.last_obs, self.last_action, reward, next_obs, done, hidden_detached)\n",
    "\n",
    "        # If the episode ended, reset the hidden state.\n",
    "        if done:\n",
    "            self.hidden = self.policy_net.init_hidden(batch_size=1)\n",
    "\n",
    "        # ---- 3. Train the DRQN if enough transitions have been collected ----\n",
    "        if len(self.replay_buffer) >= self.batch_size:\n",
    "            transitions = self.replay_buffer.sample(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            \n",
    "            # Each stored observation is shape (1, 1, input_dim); concatenate them along the batch dimension.\n",
    "            obs_batch = torch.cat(batch.obs, dim=0)       # shape: (batch, 1, input_dim)\n",
    "            actions_batch = torch.tensor(batch.action, dtype=torch.long)\n",
    "            rewards_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "            next_obs_batch = torch.cat(batch.next_obs, dim=0)  # shape: (batch, 1, input_dim)\n",
    "            dones_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "            \n",
    "            # For training we reinitialize the hidden state for the whole batch.\n",
    "            hidden_batch = self.policy_net.init_hidden(batch_size=self.batch_size)\n",
    "            \n",
    "            # Compute Q-values for the current observations.\n",
    "            q_values, _ = self.policy_net(obs_batch, hidden_batch)\n",
    "            # Select Q-values for the actions that were taken.\n",
    "            state_action_values = q_values.gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute Q-values for next observations using the target network.\n",
    "            next_hidden_batch = self.target_net.init_hidden(batch_size=self.batch_size)\n",
    "            next_q_values, _ = self.target_net(next_obs_batch, next_hidden_batch)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            \n",
    "            # Compute TD target: reward + gamma * max_next_q_value (if not done)\n",
    "            expected_state_action_values = rewards_batch + (1 - dones_batch) * self.gamma * max_next_q_values\n",
    "            \n",
    "            # Compute loss (mean squared error).\n",
    "            loss = nn.functional.mse_loss(state_action_values, expected_state_action_values.detach())\n",
    "            \n",
    "            # Optimize the policy network.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # ---- 4. Update epsilon and (for simplicity) update the target network every step. ----\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Agent Submission\n",
    "# -------------------------\n",
    "name = \"DRQNAgent\"\n",
    "nrl_agent_submission = DRQNAgent(name)\n",
    "\n",
    "# NUM_POSSIBLE_STATES = 144\n",
    "# INITIAL_STATE = 0\n",
    "# # Lemonade as 12 possible actions [0 - 11]\n",
    "# NUM_POSSIBLE_ACTIONS = 12\n",
    "# LEARNING_RATE = 0.05\n",
    "# DISCOUNT_FACTOR = 0.90\n",
    "# EXPLORATION_RATE = 0.05\n",
    "# ################### SUBMISSION #####################\n",
    "# Q_agent1 = MyRLAgent(\"Q1\", NUM_POSSIBLE_STATES, NUM_POSSIBLE_ACTIONS,\n",
    "#                                    INITIAL_STATE, LEARNING_RATE, DISCOUNT_FACTOR, EXPLORATION_RATE, False, \"my-qtable.npy\")\n",
    "# Q_agent2 = MyRLAgent(\"Q2\", NUM_POSSIBLE_STATES, NUM_POSSIBLE_ACTIONS,\n",
    "#                                    INITIAL_STATE, LEARNING_RATE, DISCOUNT_FACTOR, EXPLORATION_RATE, False, \"my-qtable.npy\")\n",
    "if __name__ == \"__main__\":\n",
    "    arena = LemonadeArena(\n",
    "        num_rounds=1000,\n",
    "        timeout=1,\n",
    "        players=[\n",
    "            nrl_agent_submission,\n",
    "            # ReserveAgent(\"reserve\"),\n",
    "            DecrementAgent(\"decrese\"),\n",
    "            StickAgent(\"stick\"),\n",
    "        ]\n",
    "    )\n",
    "    arena.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should Observe that your DRQN agent starts colluding with the stick agent by play 0 or 1 most often!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
